{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEARISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "import agrege_troncon as at\n",
    "import Connexion_Transfert as ct\n",
    "import matplotlib, os, fiona\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sqlalchemy import Table, Column, Integer, String, Float,MetaData\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely.wkt import dumps\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD17\n",
    "Réutiliser les comptages importés grace au Notebooj Import_trafics (chap cd17) et les données de troncons elemnetaires issues du notebook agrege_troncon pour déterminer les nouveaux id_comptag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des données de troncon elem (à modifier par imporft depuis Bdd quand le transfert vers Bdd en sortie d'agreg_troncon sera calé)\n",
    "gdf_traf=gp.read_file(r'D:\\temp\\otv\\test_linearisation\\df_lignes_fin_tot.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des points de comptages depuis la Bdd\n",
    "with ct.ConnexionBdd('gti_otv') as c :\n",
    "    rqt=\"select * from comptage.na_2010_2017_p where dep='17' and geom is not null\"\n",
    "    gdf_pt_cpt = gp.read_postgis(rqt, c.connexionPsy)\n",
    "#mise en forme attributs\n",
    "def convert_tmja(df) : \n",
    "    for annee in [a for a in range(2017, 2009,-1)]+['autre'] :\n",
    "        attr='tmja_'+str(annee)\n",
    "        if ~np.isnan(df[attr]) : \n",
    "            return (df[attr], annee)\n",
    "df_adj=gdf_pt_cpt.apply(lambda x : convert_tmja(x), axis=1,result_type='expand')\n",
    "df_adj.columns=['tmja_recent','annee_tmja_recent']\n",
    "gdf_pt_cpt=gdf_pt_cpt.merge(df_adj, left_index=True, right_index=True)#[['id_comptag','geom','type_poste','tmja_recent','annee_tmja_recent' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtre des points ponctuels en été\n",
    "pt_pctuels_ete=gdf_pt_cpt.loc[gdf_pt_cpt['type_poste']=='ponctuel'].copy()\n",
    "def verif_ete(tmja_2015, obs_2015, tmja_2016, obs_2016) :\n",
    "    if tmja_2016 > 0 :\n",
    "        return any([a in ['July','August']  for a in [pd.to_datetime(obs_2016.split(',')[1].split('-')[i], format='%d/%m/%Y').month_name()for i in [0,1]]])\n",
    "    elif tmja_2015 > 0 :\n",
    "        return any([a in ['July','August']  for a in [pd.to_datetime(obs_2015.split(',')[1].split('-')[i], format='%d/%m/%Y').month_name()for i in [0,1]]])\n",
    "    else : return False\n",
    "pt_pctuels_ete['ete']=pt_pctuels_ete.apply(lambda x : verif_ete(x['tmja_2015'],x['obs_2015'], x['tmja_2016'], x['obs_2016']), axis=1)\n",
    "pt_pctuels_ete=pt_pctuels_ete.loc[~pt_pctuels_ete['ete']].copy()\n",
    "gdf_pt_cpt=gdf_pt_cpt.loc[gdf_pt_cpt.index.isin(pt_pctuels_ete.index.tolist()+gdf_pt_cpt.loc[gdf_pt_cpt['type_poste']!='ponctuel'].index.tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#croisement point et lignes (sur labase d'un buffer de 0.5m)\n",
    "gdf_pt_cpt.geometry=gdf_pt_cpt.buffer(0.5)\n",
    "gdf_traf_pt=gp.sjoin(gdf_traf,gdf_pt_cpt,how=\"left\", op='intersects' )[['id_ign','tmja', 'importance',\n",
    "            'id_comptag_left', 'id_tronc_e', 'id_comptag_right','type_poste','tmja_recent','annee_tmja_recent','source','target', 'geometry']].rename(\n",
    "columns={'id_comptag_left':'id_cpt_lin','id_comptag_right': 'id_cpt_pt','tmja':'tmja_lin'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recherch des tronc_elem avec plusieurs points\n",
    "gdf_traf_pt_notna=gdf_traf_pt.loc[~gdf_traf_pt['id_cpt_pt'].isna()].copy()\n",
    "tronc_elem_pt_cpt=gdf_traf_pt_notna.sort_index().groupby('id_tronc_e').agg({'id_cpt_pt' : lambda x : tuple(x),\n",
    "                                       'type_poste' : lambda x : tuple(x),\n",
    "                                        'tmja_recent' : lambda x : tuple(x),\n",
    "                                        'annee_tmja_recent' : lambda x : tuple(x)})\n",
    "trc_elem_multipt=tronc_elem_pt_cpt.loc[tronc_elem_pt_cpt.apply(lambda x : len(x['id_cpt_pt'])>1,axis=1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mise en forme des trocnon supportant plusieurs point : \n",
    "    #gestion des trocnon avec plusieurs points differntes (on garde le ponctuele de trafic max de l'annee la plus recente si que des ponctuels, sinon le tournant ou permanent)\n",
    "    #verif de la validite des points en comprant les valeurs de trafic entre elle: si un des trafics est sup 1000 et que la variation est sup 30 %, on n'affecte pas\n",
    "def cpt_final (type_poste,id_comptag_right,tmja_recent,annee_tmja_recent ): \n",
    "    tmja_recent=np.array(tmja_recent) \n",
    "    id_comptag_right=np.array(id_comptag_right)\n",
    "    mask_tmja = (tmja_recent==max(tmja_recent))\n",
    "    if all(np.unique(type_poste)=='ponctuel') :\n",
    "        mask_annee=np.array(annee_tmja_recent)==max(annee_tmja_recent)\n",
    "        if all(mask_annee):#il n'y a qu'une seule annee\n",
    "            if all(mask_tmja) : \n",
    "                return id_comptag_right[0]\n",
    "            else : \n",
    "                return id_comptag_right[mask_tmja][0]\n",
    "        else : #il y a pluseurs annee dc on ne garde que le max de la bonne annee\n",
    "            return id_comptag_right[mask_tmja]\n",
    "    else :\n",
    "        mask_type_poste=np.isin(np.array(type_poste),['permanent', 'tournant'])#on trouve le(s) poste(s) non ponctuel\n",
    "        return id_comptag_right[mask_type_poste][0]\n",
    "#verif de la validite des points en comprant les valeurs de trafic entre elle: si un des trafics est sup 1000 et que la variation est sup 30 %, on n'affecte pas\n",
    "trc_elem_multipt['id_cpt_final']=trc_elem_multipt.apply(lambda x: cpt_final(\n",
    "    x['type_poste'], x['id_cpt_pt'], x['tmja_recent'], x['annee_tmja_recent']), axis=1)\n",
    "trc_elem_multipt['valide']=trc_elem_multipt.apply(lambda x : (False if abs(100-(min(x['tmja_recent'])/max(x['tmja_recent'])*100))>30 and\n",
    "                                                                    max(x['tmja_recent'])>1000 and all(np.unique(x['type_poste'])=='ponctuel') else True), axis=1)\n",
    "trc_elem_multipt=trc_elem_multipt.loc[trc_elem_multipt['valide']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtention de la df des id_comptage_pt par tronc_elem\n",
    "tronc_elem_pt_cpt.update(trc_elem_multipt.drop('id_cpt_pt',axis=1).rename(columns={'id_cpt_final':'id_cpt_pt'})[['id_cpt_pt']])\n",
    "tronc_elem_pt_cpt['id_cpt_pt']=tronc_elem_pt_cpt.apply(lambda x : x['id_cpt_pt'][0] if (isinstance(x['id_cpt_pt'], tuple) or \n",
    "                                                       isinstance(x['id_cpt_pt'], np.ndarray)) else x['id_cpt_pt'],axis=1) #uniformisation du type de données en str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfert de l'id_cpt_pt final dans la df source\n",
    "gdf_traf_pt_final=gdf_traf_pt[['id_ign','importance', 'id_cpt_lin','tmja_lin', 'id_tronc_e','geometry','source','target']].merge(tronc_elem_pt_cpt[['id_cpt_pt']], how='left',left_on='id_tronc_e', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour info, ligne ign non linéarisée qui le deviennent\n",
    "gdf_creation_idcpt=gdf_traf_pt_final.loc[(gdf_traf_pt_final['id_cpt_lin'].isna()) & (~gdf_traf_pt_final['id_cpt_pt'].isna())]\n",
    "#pour info, ligne ign linéarisée qui le sont toujours, mais peut avec un nouvel id_ign\n",
    "gdf_variation_idcpt=gdf_traf_pt_final.loc[(~gdf_traf_pt_final['id_cpt_lin'].isna()) & (~gdf_traf_pt_final['id_cpt_pt'].isna()) & (gdf_traf_pt_final['id_cpt_pt']!=gdf_traf_pt_final['id_cpt_lin'])].sort_values('id_cpt_lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mettre à jour les lignes des troncons qui appartiennent à des id_comptag qui supporte un nouvel id_cpt\n",
    "**Dans un premier temps on ne traite que les points de comptage de l'otv pour lesquels le nouveau point de comptage présente le mm nom de voie**\n",
    "exemple avec l'idcomptag '17-D734-19+620' pour le point de comptage nouveau '17-D734-28+170'\n",
    "1. isoler les lignes qui sont relaticves à l'id_cpt_lin et trouver l'importance la plus représentée\n",
    "1. trouver les lignes qui intersectent celles de l'idcompatg etudie dont l'importance est = ou sup\n",
    "1. faire un graph avec les lignes de l'id_cpt_lin et celles qui interectent avec la bonne importance\n",
    "1. trouver toute les lignes concernées par l'id_cpt_pt à partir du nouveau graph (plus tard quand on trouvera facilement les troncons elemenraires cela ne sera plus utile) comme ça on ne s'embarasse pas des pb de croisement avec des lignes de moindre importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17-A10-423+0\n",
      "17-D1-4+0\n",
      "17-D104-4+310\n",
      "17-D108-4+264\n",
      "17-D109-13+0\n",
      "17-D114-54+450\n",
      "17-D114-64+900\n",
      "17-D115-31+170\n",
      "17-D116-31+0\n",
      "17-D116-40+600\n",
      "17-D117-18+415\n",
      "17-D123-15+800\n",
      "17-D124-3+500\n",
      "17-D124-42+580\n",
      "17-D126-15+660\n",
      "17-D129-20+340\n",
      "17-D131-39+440\n",
      "17-D137-1+949\n",
      "17-D137-13+50\n",
      "17-D137-132+136\n",
      "17-D137-44+377\n",
      "17-D137-72+18\n",
      "17-D14-2+500\n",
      "17-D140-7+184\n",
      "17-D141-3+600\n",
      "17-D145-33+863\n",
      "17-D150-1+28\n",
      "17-D150-27+700\n",
      "17-D158E1-7+0\n",
      "17-D17-8+0\n",
      "17-D18-25+103\n",
      "17-D19-8+669\n",
      "17-D2-21+729\n",
      "17-D2-6+750\n",
      "17-D20-2+185\n",
      "17-D201-2+800\n",
      "17-D728-0+460\n",
      "17-D730-9+939\n",
      "17-D731-7+600\n",
      "17-D733-29+497\n",
      "17-D734-19+620\n",
      "17-D734-5+600\n",
      "17-D735-13+313\n",
      "17-D739-8+0\n",
      "17-D9-15+485\n",
      "17-D9-9+670\n",
      "pb sur id_comptag : f{id_cpt_a_test}\n",
      "17-D938T-1+685\n",
      "17-D939-47+803\n",
      "17-D939-69+0\n",
      "17-D939-88+0\n",
      "17-D950-1+900\n",
      "17-D950-16+246\n"
     ]
    }
   ],
   "source": [
    "#on ne prend que les points de comptages présentant le mm nom de voie\n",
    "gdf_variation_idcpt_ok=gdf_variation_idcpt.loc[gdf_variation_idcpt.apply(lambda x : \n",
    "                        x['id_cpt_lin'].split('-')[1] == x['id_cpt_pt'].split('-')[1],axis=1)].copy()\n",
    "\n",
    "#isoler les lignes qui sont relaticves à l'id_cpt_lin et trouver l'importance la plus représentée\n",
    "for id_cpt_a_test in gdf_variation_idcpt_ok.id_cpt_lin.unique() : \n",
    "    print(id_cpt_a_test)\n",
    "    #id_cpt_a_test='17-D108-4+264'\n",
    "    lgn_idcpt_lin=gdf_traf.loc[gdf_traf['id_comptag']==id_cpt_a_test].copy()\n",
    "    importance=[k for k, v in Counter(lgn_idcpt_lin.importance.tolist()).items() if v==max( Counter(lgn_idcpt_lin.importance.tolist()).values())][0]\n",
    "\n",
    "    #trouver les lignes qui intersectent celles de l'idcompatg etudie dont l'importance est = ou sup\n",
    "        #list des source / target de l'idcomptage\n",
    "    list_src_tgt=lgn_idcpt_lin.source.tolist()+lgn_idcpt_lin.target.tolist()\n",
    "    df_lgn_interscts=gdf_traf.loc[((gdf_traf['source'].isin(list_src_tgt)) | (gdf_traf['target'].isin(list_src_tgt))) & (gdf_traf['importance'] <=importance) &\n",
    "                                 (~gdf_traf['id_ign'].isin(lgn_idcpt_lin.id_ign.tolist()))]\n",
    "\n",
    "    #faire un graph avec les lignes de l'id_cpt_lin et celles qui interectent avec la bonne importance\n",
    "        #regrouper l'ensemble des lignes dans une df \n",
    "    ligne_pr_graph=gdf_traf.loc[gdf_traf['id_ign'].isin(df_lgn_interscts.id_ign.tolist()+lgn_idcpt_lin.id_ign.tolist())]\n",
    "    ligne_pr_graph_transfert=ligne_pr_graph.copy()\n",
    "    ligne_pr_graph_transfert['geom']=ligne_pr_graph_transfert.apply(lambda x : dumps(x.geometry), axis=1)\n",
    "        #faire un graph à partir de ces lignes\n",
    "    bdd='gti_otv'\n",
    "    schema='public'\n",
    "    table='graph_temp'\n",
    "    table_vertex='graph_temp_vertices_pgr'\n",
    "    with ct.ConnexionBdd(bdd) as c:\n",
    "        metadata = MetaData(schema=schema)\n",
    "        #supprimer table si elle existe\n",
    "        rqt=f\"drop table if exists {schema}.{table} ; drop table if exists {schema}.{table_vertex} \"\n",
    "        c.sqlAlchemyConn.execute(rqt)\n",
    "\n",
    "        #creer nouvelle table\n",
    "        graph = Table('graph_temp', metadata,\n",
    "            Column('id', Integer, primary_key=True),\n",
    "            Column('id_ign', String),\n",
    "            Column('importance', String),\n",
    "            Column('numero', String),\n",
    "            Column('nature', String),\n",
    "            Column('codevoie_d', String),\n",
    "            Column('sens', String),\n",
    "            Column('tmja', Integer),\n",
    "            Column('source', Integer),\n",
    "            Column('target', Integer),\n",
    "            Column('long_km', Float),\n",
    "            Column('geom',Geometry()))\n",
    "        metadata.create_all(c.engine)\n",
    "        ligne_pr_graph_transfert[['id','id_ign','importance','numero','nature','codevoie_d','tmja','source','sens','target','long_km','geom']].to_sql(\n",
    "            table,c.sqlAlchemyConn,schema=schema,if_exists='append', index=False )\n",
    "        #creer le graph\n",
    "        rqt_creation_graph=f\"\"\"update {schema}.{table} set geom=st_Multi(st_setsrid(geom,2154)), source=null, target=null ; \n",
    "                             select pgr_createTopology('{schema}.{table}', 0.001,'geom')\"\"\"\n",
    "        c.sqlAlchemyConn.execute(rqt_creation_graph)\n",
    "        rqt_anlyse_graph=f\"SELECT pgr_analyzeGraph('{schema}.{table}', 0.001,\\'geom\\')\"\n",
    "        c.curs.execute(rqt_anlyse_graph)#je le fait avec psycopg2 car avec sql acchemy ça ne passe pas\n",
    "        c.connexionPsy.commit()\n",
    "    #appel des fonction de carac des lignes \n",
    "    df=at.import_donnes_base('gti_otv','public', 'graph_temp','graph_temp_vertices_pgr')\n",
    "    df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "    df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=at.identifier_rd_pt(df)\n",
    "    df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "\n",
    "    #trouver une ligne relative à chaque nouveau point\n",
    "    liste_new_pt=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_lin']==id_cpt_a_test].id_cpt_pt.unique()\n",
    "    #ensuite l'idée c'est de créer un dico avec pr chauqe point de comptage que l'on souhaite lineariser le recenesmeent de  : \n",
    "    #la ligne qui supporte le pt, le numero du tronc elem, les lignes qui compose le tronc elem\n",
    "    #cela va permettre de recerche pr chaque point de comptage les lignes du mm tronc_elem, qui ne sont pas das les tronc_elem des autres\n",
    "    gdf_variation_idcpt_ok_geom=gdf_variation_idcpt_ok.merge(gdf_pt_cpt[['id_comptag', 'geom']], left_on='id_cpt_pt', right_on='id_comptag').rename(columns={\n",
    "        'geom' : 'geom_id_cpt_pt'}) # pour trouver la ligne qui suppotrt ele pt j'ai besoin da le distance au pt de comptage\n",
    "    gdf_variation_idcpt_ok_geom['dist_id_cpt']=gdf_variation_idcpt_ok_geom.apply(lambda x : x['geometry'].distance(x['geom_id_cpt_pt']), axis=1)\n",
    "\n",
    "    if len(liste_new_pt)==1 :\n",
    "        liste_new_pt=liste_new_pt[0]\n",
    "        t1=gdf_variation_idcpt_ok_geom.loc[(gdf_variation_idcpt_ok_geom['id_cpt_pt']==liste_new_pt)]\n",
    "        dico_ligne_pt_base=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']].drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                        'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "\n",
    "        #dico_ligne_pt_base=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_pt']==liste_new_pt].groupby('id_cpt_pt').agg({'id_ign': 'max',\n",
    "                                            #'id_tronc_e' : 'max'}).to_records()\n",
    "    else : \n",
    "        t1=gdf_variation_idcpt_ok_geom.loc[(gdf_variation_idcpt_ok_geom['id_cpt_pt'].isin(liste_new_pt))]\n",
    "        dico_ligne_pt_base=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']].drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                        'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "        #dico_ligne_pt_base=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_pt'].isin(liste_new_pt)].groupby('id_cpt_pt').agg({'id_ign': 'max',\n",
    "                                            #'id_tronc_e' : 'max'}).to_records()\n",
    "    dico_ligne_pt={k[0] : {'ligne_base': k[1],'trc_elem':k[2],'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==k[2]) & \n",
    "                                                                            (gdf_traf_pt.id_ign.isin(df_lignes.index.tolist()))].id_ign.tolist(), 'priorite':2} \n",
    "                   for k in dico_ligne_pt_base}\n",
    "    dico_ligne_pt[id_cpt_a_test]={'ligne_base' : gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                                (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_ign.values[0],\n",
    "                                 'trc_elem':gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                                (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0], \n",
    "                                 'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(\n",
    "                                     ['permanent','tournant'])) & (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0]) & \n",
    "                   (gdf_traf_pt['id_ign'].isin(df_lignes.index.to_list()))].id_ign.tolist(),\n",
    "                                 'priorite':1}\n",
    "        #dico des lignes relatives a un autre comptage\n",
    "    dico_lign_cpt_pct={k:[a for kd in dico_ligne_pt.keys() if kd!=k for a in dico_ligne_pt[kd]['lgn_trc_elem']] for k in dico_ligne_pt.keys()}\n",
    "\n",
    "    #attention, la fonction at.regrouper_troncon ne marche pas si la ligne de base est un rd point.\n",
    "    #il faut dc vérifier ce point et dans le cas où ça arrive prendre un ligne des lgn_trc_elem qui ne soit pas sur un rdpoint\n",
    "    if not carac_rd_pt.empty : \n",
    "        list_id_ign_rd_pt=[a for l_rdpt in carac_rd_pt.id_ign_rdpt.tolist() for a in l_rdpt]\n",
    "        for k in dico_ligne_pt.keys() : \n",
    "            if dico_ligne_pt[k]['ligne_base'] in list_id_ign_rd_pt : \n",
    "                dico_ligne_pt[k]['ligne_base']=[e for e in dico_ligne_pt[k]['lgn_trc_elem'] if e not in list_id_ign_rd_pt][0]\n",
    "\n",
    "    #pour chaque ligne : on fait tourner la fonction de tronc elem à partir du graph avec les importance sup ou égale, \n",
    "    #d'abord pour le cpt permanent, ensuite pour les autres, avec prise necompte des lignes des tronc elem des autres compteurs. \n",
    "        #list des id_ign des tronc_elem\n",
    "    liste_id_ign_te=[a  for k, v in dico_ligne_pt.items() for a in v['lgn_trc_elem']]\n",
    "    lignes_traitees=[]\n",
    "\n",
    "    #utilisret les fnctions de carac\n",
    "    try : \n",
    "        for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])): \n",
    "            ligne_comp=[x for x in at.regrouper_troncon([dico_ligne_pt[k]['ligne_base']],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "                                                       dico_lign_cpt_pct[k]+lignes_traitees)[0].id.tolist() \n",
    "                        if x in df_lignes.index.tolist()]\n",
    "            #print(f\"id_cpt : {k},'\\n',lignes_traitees : {lignes_traitees},'\\n', lignes de base : {ligne_comp}\")\n",
    "            lignes_traitees+=ligne_comp\n",
    "            dico_ligne_pt[k]['lignes_comp_base']=ligne_comp\n",
    "    except KeyError : \n",
    "        print(f'pb sur id_comptag : {id_cpt_a_test}')\n",
    "\n",
    "    #a ce stade, si le cpt perm est séparé par une voie d'importance sup ou =, il y a discontinuité, dc il faut \n",
    "        #-relevé les discontinuite\n",
    "\n",
    "    # mettre en forme et creation df de correspondance finale : \n",
    "    df_linearisee=pd.DataFrame(index=dico_ligne_pt.keys(), data=dico_ligne_pt.values())\n",
    "    df_linearisee['lignes_comp_fin']=df_linearisee.apply(lambda x : x['lignes_comp_base'] + x['lgn_trc_elem'], axis=1 )\n",
    "    dico_inverse=df_linearisee[['lignes_comp_fin']].to_dict()\n",
    "    dico_inverse={a:k   for k, val  in dico_inverse['lignes_comp_fin'].items() for a in val}\n",
    "    df_linearisee_fin=pd.DataFrame(index=dico_inverse.keys(), data=dico_inverse.values(), columns=['id_cpt_fin'])\n",
    "\n",
    "    try : \n",
    "        df_linearisee_fin_glob=pd.concat([df_linearisee_fin_glob,df_linearisee_fin], axis=0, sort=False)\n",
    "    except NameError : \n",
    "        df_linearisee_fin_glob=df_linearisee_fin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f df_linearisee_fin_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre à jour l'id_cpt dans la df de base\n",
    "gdf_traf_upd=gdf_traf.set_index('id_ign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linearisee_fin_glob.to_csv(r'D:\\temp\\otv\\test_linearisation\\test_lin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on ne prend que les points de comptages présentant le mm nom de voie\n",
    "gdf_variation_idcpt_ok=gdf_variation_idcpt.loc[gdf_variation_idcpt.apply(lambda x : \n",
    "                        x['id_cpt_lin'].split('-')[1] == x['id_cpt_pt'].split('-')[1],axis=1)].copy()\n",
    "\n",
    "#isoler les lignes qui sont relaticves à l'id_cpt_lin et trouver l'importance la plus représentée\n",
    "#for id_cpt_a_test in gdf_variation_idcpt_ok.id_cpt_lin.unique() : \n",
    "    #print(id_cpt_a_test)\n",
    "id_cpt_a_test='17-D117-18+415'\n",
    "lgn_idcpt_lin=gdf_traf.loc[gdf_traf['id_comptag']==id_cpt_a_test].copy()\n",
    "importance=[k for k, v in Counter(lgn_idcpt_lin.importance.tolist()).items() if v==max( Counter(lgn_idcpt_lin.importance.tolist()).values())][0]\n",
    "\n",
    "#trouver les lignes qui intersectent celles de l'idcompatg etudie dont l'importance est = ou sup\n",
    "    #list des source / target de l'idcomptage\n",
    "list_src_tgt=lgn_idcpt_lin.source.tolist()+lgn_idcpt_lin.target.tolist()\n",
    "df_lgn_interscts=gdf_traf.loc[((gdf_traf['source'].isin(list_src_tgt)) | (gdf_traf['target'].isin(list_src_tgt))) & (gdf_traf['importance'] <=importance) &\n",
    "                             (~gdf_traf['id_ign'].isin(lgn_idcpt_lin.id_ign.tolist()))]\n",
    "\n",
    "#faire un graph avec les lignes de l'id_cpt_lin et celles qui interectent avec la bonne importance\n",
    "    #regrouper l'ensemble des lignes dans une df \n",
    "ligne_pr_graph=gdf_traf.loc[gdf_traf['id_ign'].isin(df_lgn_interscts.id_ign.tolist()+lgn_idcpt_lin.id_ign.tolist())]\n",
    "ligne_pr_graph_transfert=ligne_pr_graph.copy()\n",
    "ligne_pr_graph_transfert['geom']=ligne_pr_graph_transfert.apply(lambda x : dumps(x.geometry), axis=1)\n",
    "    #faire un graph à partir de ces lignes\n",
    "bdd='gti_otv'\n",
    "schema='public'\n",
    "table='graph_temp'\n",
    "table_vertex='graph_temp_vertices_pgr'\n",
    "with ct.ConnexionBdd(bdd) as c:\n",
    "    metadata = MetaData(schema=schema)\n",
    "    #supprimer table si elle existe\n",
    "    rqt=f\"drop table if exists {schema}.{table} ; drop table if exists {schema}.{table_vertex} \"\n",
    "    c.sqlAlchemyConn.execute(rqt)\n",
    "\n",
    "    #creer nouvelle table\n",
    "    graph = Table('graph_temp', metadata,\n",
    "        Column('id', Integer, primary_key=True),\n",
    "        Column('id_ign', String),\n",
    "        Column('importance', String),\n",
    "        Column('numero', String),\n",
    "        Column('nature', String),\n",
    "        Column('codevoie_d', String),\n",
    "        Column('sens', String),\n",
    "        Column('tmja', Integer),\n",
    "        Column('source', Integer),\n",
    "        Column('target', Integer),\n",
    "        Column('long_km', Float),\n",
    "        Column('geom',Geometry()))\n",
    "    metadata.create_all(c.engine)\n",
    "    ligne_pr_graph_transfert[['id','id_ign','importance','numero','nature','codevoie_d','tmja','source','sens','target','long_km','geom']].to_sql(\n",
    "        table,c.sqlAlchemyConn,schema=schema,if_exists='append', index=False )\n",
    "    #creer le graph\n",
    "    rqt_creation_graph=f\"\"\"update {schema}.{table} set geom=st_Multi(st_setsrid(geom,2154)), source=null, target=null ; \n",
    "                         select pgr_createTopology('{schema}.{table}', 0.001,'geom')\"\"\"\n",
    "    c.sqlAlchemyConn.execute(rqt_creation_graph)\n",
    "    rqt_anlyse_graph=f\"SELECT pgr_analyzeGraph('{schema}.{table}', 0.001,\\'geom\\')\"\n",
    "    c.curs.execute(rqt_anlyse_graph)#je le fait avec psycopg2 car avec sql acchemy ça ne passe pas\n",
    "    c.connexionPsy.commit()\n",
    "#appel des fonction de carac des lignes \n",
    "df=at.import_donnes_base('gti_otv','public', 'graph_temp','graph_temp_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=at.identifier_rd_pt(df)\n",
    "df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "\n",
    "#trouver une ligne relative à chaque nouveau point\n",
    "liste_new_pt=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_lin']==id_cpt_a_test].id_cpt_pt.unique()\n",
    "#ensuite l'idée c'est de créer un dico avec pr chauqe point de comptage que l'on souhaite lineariser le recenesmeent de  : \n",
    "#la ligne qui supporte le pt, le numero du tronc elem, les lignes qui compose le tronc elem\n",
    "#cela va permettre de recerche pr chaque point de comptage les lignes du mm tronc_elem, qui ne sont pas das les tronc_elem des autres\n",
    "gdf_variation_idcpt_ok_geom=gdf_variation_idcpt_ok.merge(gdf_pt_cpt[['id_comptag', 'geom']], left_on='id_cpt_pt', right_on='id_comptag').rename(columns={\n",
    "    'geom' : 'geom_id_cpt_pt'}) # pour trouver la ligne qui suppotrt ele pt j'ai besoin da le distance au pt de comptage\n",
    "gdf_variation_idcpt_ok_geom['dist_id_cpt']=gdf_variation_idcpt_ok_geom.apply(lambda x : x['geometry'].distance(x['geom_id_cpt_pt']), axis=1)\n",
    "\n",
    "if len(liste_new_pt)==1 :\n",
    "    liste_new_pt=liste_new_pt[0]\n",
    "    t1=gdf_variation_idcpt_ok_geom.loc[(gdf_variation_idcpt_ok_geom['id_cpt_pt']==liste_new_pt)]\n",
    "    dico_ligne_pt_base=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']].drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                    'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "\n",
    "    #dico_ligne_pt_base=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_pt']==liste_new_pt].groupby('id_cpt_pt').agg({'id_ign': 'max',\n",
    "                                        #'id_tronc_e' : 'max'}).to_records()\n",
    "else : \n",
    "    t1=gdf_variation_idcpt_ok_geom.loc[(gdf_variation_idcpt_ok_geom['id_cpt_pt'].isin(liste_new_pt))]\n",
    "    dico_ligne_pt_base=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']].drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                    'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "    #dico_ligne_pt_base=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_pt'].isin(liste_new_pt)].groupby('id_cpt_pt').agg({'id_ign': 'max',\n",
    "                                        #'id_tronc_e' : 'max'}).to_records()\n",
    "dico_ligne_pt={k[0] : {'ligne_base': k[1],'trc_elem':k[2],'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==k[2]) & \n",
    "                                                                        (gdf_traf_pt.id_ign.isin(df_lignes.index.tolist()))].id_ign.tolist(), 'priorite':2} \n",
    "               for k in dico_ligne_pt_base}\n",
    "dico_ligne_pt[id_cpt_a_test]={'ligne_base' : gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                            (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_ign.values[0],\n",
    "                             'trc_elem':gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                            (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0], \n",
    "                             'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(\n",
    "                                 ['permanent','tournant'])) & (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0]) & \n",
    "               (gdf_traf_pt['id_ign'].isin(df_lignes.index.to_list()))].id_ign.tolist(),\n",
    "                             'priorite':1}\n",
    "    #dico des lignes relatives a un autre comptage\n",
    "dico_lign_cpt_pct={k:[a for kd in dico_ligne_pt.keys() if kd!=k for a in dico_ligne_pt[kd]['lgn_trc_elem']] for k in dico_ligne_pt.keys()}\n",
    "\n",
    "#attention, la fonction at.regrouper_troncon ne marche pas si la ligne de base est un rd point.\n",
    "#il faut dc vérifier ce point et dans le cas où ça arrive prendre un ligne des lgn_trc_elem qui ne soit pas sur un rdpoint\n",
    "if not carac_rd_pt.empty : \n",
    "    list_id_ign_rd_pt=[a for l_rdpt in carac_rd_pt.id_ign_rdpt.tolist() for a in l_rdpt]\n",
    "    for k in dico_ligne_pt.keys() : \n",
    "        if dico_ligne_pt[k]['ligne_base'] in list_id_ign_rd_pt : \n",
    "            dico_ligne_pt[k]['ligne_base']=[e for e in dico_ligne_pt[k]['lgn_trc_elem'] if e not in list_id_ign_rd_pt][0]\n",
    "\n",
    "#pour chaque ligne : on fait tourner la fonction de tronc elem à partir du graph avec les importance sup ou égale, \n",
    "#d'abord pour le cpt permanent, ensuite pour les autres, avec prise necompte des lignes des tronc elem des autres compteurs. \n",
    "    #list des id_ign des tronc_elem\n",
    "liste_id_ign_te=[a  for k, v in dico_ligne_pt.items() for a in v['lgn_trc_elem']]\n",
    "lignes_traitees=[]\n",
    "\n",
    "#utilisret les fnctions de carac\n",
    "try : \n",
    "    for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])): \n",
    "        ligne_comp=[x for x in at.regrouper_troncon([dico_ligne_pt[k]['ligne_base']],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "                                                   dico_lign_cpt_pct[k]+lignes_traitees)[0].id.tolist() \n",
    "                    if x in df_lignes.index.tolist()]\n",
    "        #print(f\"id_cpt : {k},'\\n',lignes_traitees : {lignes_traitees},'\\n', lignes de base : {ligne_comp}\")\n",
    "        lignes_traitees+=ligne_comp\n",
    "        dico_ligne_pt[k]['lignes_comp_base']=ligne_comp\n",
    "except KeyError : \n",
    "    print(f'pb sur id_comptag : {id_cpt_a_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a ce stade, si le cpt perm est séparé par une voie d'importance sup ou =, il y a discontinuité, dc il faut \n",
    "    #-relevé les discontinuite\n",
    "ids_affectes=[a for k in dico_ligne_pt.keys() for a in dico_ligne_pt[k]['lignes_comp_base']]\n",
    "df_non_affecte=lgn_idcpt_lin.loc[~lgn_idcpt_lin.id_ign.isin(ids_affectes)]\n",
    "    #tant que des discontexistent\n",
    "while not df_non_affecte.empty : \n",
    "    for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])):\n",
    "        #discontinuité qui touche lecpt perm : \n",
    "        src_tgt_affecte=( lgn_idcpt_lin.loc[lgn_idcpt_lin.id_ign.isin(dico_ligne_pt[k]['lignes_comp_base'])].source.tolist() + \n",
    "                lgn_idcpt_lin.loc[lgn_idcpt_lin.id_ign.isin(dico_ligne_pt[k]['lignes_comp_base'])].target.tolist() )\n",
    "        ids_tch_cpt=df_non_affecte.loc[(df_non_affecte.source.isin(src_tgt_affecte)) | (df_non_affecte.target.isin(src_tgt_affecte))].id_ign.tolist()\n",
    "        #pour chaque ligne on récupère tout les troncons elem et on ajoute au dico\n",
    "        ids_a_ajouter=[a for b in [at.regrouper_troncon([e],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "             [a  for k in dico_ligne_pt.keys() for a in dico_ligne_pt[k]['lignes_comp_base']])[0].id.tolist() for e in ids_tch_cpt] for a in b]\n",
    "        dico_ligne_pt[k]['lignes_comp_base']+=ids_a_ajouter\n",
    "    ids_affectes=[a for k in dico_ligne_pt.keys() for a in dico_ligne_pt[k]['lignes_comp_base']]\n",
    "    df_non_affecte=lgn_idcpt_lin.loc[~lgn_idcpt_lin.id_ign.isin(ids_affectes)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettre en forme et creation df de correspondance finale : \n",
    "df_linearisee=pd.DataFrame(index=dico_ligne_pt.keys(), data=dico_ligne_pt.values())\n",
    "df_linearisee['lignes_comp_fin']=df_linearisee.apply(lambda x : x['lignes_comp_base'] + x['lgn_trc_elem'], axis=1 )\n",
    "dico_inverse=df_linearisee[['lignes_comp_fin']].to_dict()\n",
    "dico_inverse={a:k   for k, val  in dico_inverse['lignes_comp_fin'].items() for a in val}\n",
    "df_linearisee_fin=pd.DataFrame(index=dico_inverse.keys(), data=dico_inverse.values(), columns=['id_cpt_fin'])\n",
    "\n",
    "try : \n",
    "    df_linearisee_fin_glob=pd.concat([df_linearisee_fin_glob,df_linearisee_fin], axis=0, sort=False)\n",
    "except NameError : \n",
    "    df_linearisee_fin_glob=df_linearisee_fin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linearisee_fin_glob.to_csv(r'D:\\temp\\otv\\test_linearisation\\test_lin_uniq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not df_non_affecte.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
