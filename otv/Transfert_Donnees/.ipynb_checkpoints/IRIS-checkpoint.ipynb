{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTATION DES DONNEES DANS IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import csv, re, os, shutil\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "import Connexion_Transfert as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTATION DES DONNEES GESTIONNAIRES TYPE T6 / B6\n",
    "> Exemple du CD16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ouverture des données de stations issues de IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checker si une station est présente dans la liste des stations\n",
    "#ouvertur de la liste des stations\n",
    "with open(r'P:\\DAIT\\TI\\Donnees\\1-ROUTIER\\Logiciel-IRIS\\Liste_des_stations_2019-17-06.csv', newline='') as f :\n",
    "    freader=csv.reader(f, delimiter=';')\n",
    "    liste=[]\n",
    "    for indice,row in enumerate(freader) : \n",
    "        if indice==0 : \n",
    "            liste.append([re.sub(('é|è|ê'),'e',mot).replace('.','').replace(' ','_') for mot in row])\n",
    "        else : \n",
    "            liste.append([re.sub(('é|è|ê'),'e',mot) for mot in row])\n",
    "    df_station=pd.DataFrame(liste[1:], columns=liste[0])\n",
    "    df_station=df_station.loc[df_station.Dep.isin(['16','17','79','86','33','24','40','47','64','23','19','87'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre en forme les attributs pour comparaison : passer les lat,long et Prs station en float pour povoir comparer plus facilement (sans pb d'arrondi)\n",
    "df_station['long']=df_station.apply(lambda x : float(x['Longitude']),axis=1)\n",
    "df_station['Lat']=df_station.apply(lambda x : float(x['Latitude']),axis=1)\n",
    "df_station['pr']=df_station.apply(lambda x : float(x['Pr_station'].split('.')[0]),axis=1)\n",
    "df_station['abs']=df_station.apply(lambda x : float(x['Pr_station'].split('.')[1]),axis=1)\n",
    "df_station['dep']=df_station.apply(lambda x : int(x['Dep']),axis=1)\n",
    "df_station['route']=df_station.apply(lambda x : x['Routes'].replace(' ',''),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifier les stations en doublons\n",
    "df_station_16.sort_values('Id_reel').loc[df_station_16.duplicated('Id_reel',False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Test des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pb reference station sur 17BIRACT6TV02.csv\n",
      "pb reference station sur 17BIRACT6TV01.csv\n"
     ]
    }
   ],
   "source": [
    "#pour chaque fichier on vérifie que les données de station sont connues, complete et que les fichiers de sens 1 et 2 sont dispos\n",
    "for root, dir, files in os.walk(chemin_src) : \n",
    "    for file in files: \n",
    "        if file.endswith('.csv') : #on e s'interesse qu'au csv\n",
    "            if 'TV01' in file : # test sur la pérsence des fihciers de sens 1 et 2 \n",
    "                if not os.path.isfile(os.path.join(chemin_src,file.replace('TV01','TV02'))) : \n",
    "                    print(f'pas de sens 2 pour {file}')\n",
    "                    continue\n",
    "            else : \n",
    "                if not os.path.isfile(os.path.join(chemin_src,file.replace('TV02','TV01'))) : \n",
    "                    print(f'pas de sens 1 pour {file}')\n",
    "                    continue\n",
    "            with open(os.path.join(chemin_src,file), newline='') as compt :\n",
    "                compt_reader=csv.reader(compt, delimiter=';')\n",
    "                dico={indice:row for indice,row in enumerate(compt_reader)} \n",
    "                voie=dico[4][16]    \n",
    "                pr=dico[5][16].replace('+','.')\n",
    "                dep=16\n",
    "                type_voie='RD'\n",
    "                type_fichier=dico[7][6]\n",
    "                nom_reel='_'.join([str(dep),type_voie,voie,pr])\n",
    "                df_cpt=pd.DataFrame(data=[dico[key][1:] for key in range(11,len(dico)-11)],\n",
    "                                   index=[dico[key][0] for key in range(11,len(dico)-11)],\n",
    "                                  columns=dico[10][1:])\n",
    "            if nom_reel in df_station.Id_reel.tolist()  :\n",
    "                if df_cpt.loc[df_cpt.isna().any(axis=1)].empty :\n",
    "                    chemin_dst=r'P:\\DAIT\\TI\\Donnees\\1-ROUTIER\\Logiciel-IRIS\\CG16\\2017_CD16\\pour_transfert_IRIS'\n",
    "                    shutil.copyfile(os.path.join(chemin_src,file), os.path.join(chemin_dst,file))\n",
    "                else : \n",
    "                    print(f'pb donnees sur {file}')\n",
    "            else : \n",
    "                print(f'pb reference station sur {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Import des données de l'observatoire des trafics\n",
    "> L'idée est de créer un fichier qui réponde au format présenté au chapitre 5.3 de l'aide administrateur de IRIS __https://dreal.applis-bretagne.fr/iris/www/administration/FilesUploaded/pdf_admin/Notice-Administrateur.pdf__<br> Pour ça, j'import les données 2017 depuis la Bdd et j'ajoute des attributs, ensuite je fais 4 jointures successives pour obtenir d'un coté les références au station issues de IRIS et de l'autre les trafics, puis mise en forme selon la doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ct.ConnexionBdd('gti_otv') as c :\n",
    "    requete='''\n",
    "    select \n",
    "    replace(id_comptag,'+','.') as nom_station_bdd, type_poste, dep::integer as dep_bdd, reseau, route as route_bdd, pr as pr_bdd,abs as abs_bdd,\n",
    "    st_y(st_transform(st_setsrid((st_dump(geom)).geom,2154),4326))  as y_wgs,\n",
    "    st_x(st_transform(st_setsrid((st_dump(geom)).geom,2154),4326)) as x_wgs,\n",
    "    'sens1'as sens1,'sens2'as sens2, 2017 as annee, \n",
    "    round((tmja_2017*(1-(pc_pl_2017*0.01)))/2) as vl_sens_1, round(tmja_2017*pc_pl_2017/200) as pl_sens_1, \n",
    "    round((tmja_2017*(1-(pc_pl_2017*0.01)))/2) as vl_sens_2 , round(tmja_2017*pc_pl_2017/200) as pl_sens_2\n",
    "    from comptage.na_2010_2017_p\n",
    "    where concession = 'N'\n",
    "    '''\n",
    "    df_tmja_otv=pd.read_sql_query(requete, c.sqlAlchemyConn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ne conserver que les lignes pour lesquelles l'ensembles des données est renseignées\n",
    "df_tmja_otv_valide=df_tmja_otv.loc[df_tmja_otv.notna().all(axis=1)].copy()\n",
    "#creer des attributs de jointure\n",
    "df_tmja_otv_valide['Id_reel_bdd']=df_tmja_otv_valide.apply(lambda x : str(x['dep_bdd'])+'_'+x['reseau']+'_'+x['route_bdd'][1:]+'_'+x['nom_station_bdd']\n",
    "                                                           .split('-')[2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. jointure sur les dep, route,pr, abs\n",
    "jointur_pr_abs=df_station.merge(df_tmja_otv_valide, left_on=['dep','route','pr','abs'], right_on=['dep_bdd','route_bdd','pr_bdd','abs_bdd'])\n",
    "station_non_renseigne=df_station.loc[~df_station.Id.isin(jointur_pr_abs.Id.tolist())]\n",
    "df_tmja_otv_valide_nr=df_tmja_otv_valide.loc[~df_tmja_otv_valide['nom_station_bdd'].isin(jointur_pr_abs.nom_station_bdd.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. jointure sur coordonnees\n",
    "jointure_coord=station_non_renseigne.merge(df_tmja_otv_valide_nr, left_on=['Lat','long'], right_on=['y_wgs','x_wgs'])\n",
    "jointure_totale=pd.concat([jointur_pr_abs,jointure_coord],sort=False)\n",
    "station_non_renseigne=df_station.loc[~df_station.Id.isin(jointure_totale.Id.tolist())]\n",
    "df_tmja_otv_valide_nr=df_tmja_otv_valide.loc[~df_tmja_otv_valide['nom_station_bdd'].isin(jointure_totale.nom_station_bdd.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.jointure sur id_reel\n",
    "jointure_idreel=station_non_renseigne.merge(df_tmja_otv_valide_nr, left_on='Id_reel', right_on='Id_reel_bdd')\n",
    "jointure_totale=pd.concat([jointure_totale,jointure_idreel],sort=False)\n",
    "station_non_renseigne=df_station.loc[~df_station.Id.isin(jointure_totale.Id.tolist())]\n",
    "df_tmja_otv_valide_nr=df_tmja_otv_valide.loc[~df_tmja_otv_valide['nom_station_bdd'].isin(jointure_totale.nom_station_bdd.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. jointure sur abscisse proche\n",
    "jointur_abs_proche=station_non_renseigne.merge(df_tmja_otv_valide_nr, left_on=['dep','route','pr'], right_on=['dep_bdd','route_bdd','pr_bdd'])\n",
    "jointur_abs_proche_vf=jointur_abs_proche.loc[jointur_abs_proche.apply(lambda x : x['abs_bdd']-20<x['abs']<x['abs_bdd']+20, axis=1)].copy()\n",
    "jointure_totale=pd.concat([jointure_totale,jointur_abs_proche_vf],sort=False)\n",
    "station_non_renseigne=df_station.loc[~df_station.Id.isin(jointure_totale.Id.tolist())]\n",
    "df_tmja_otv_valide_nr=df_tmja_otv_valide.loc[~df_tmja_otv_valide['nom_station_bdd'].isin(jointure_totale.nom_station_bdd.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettre en forme pour export csv\n",
    "Fichier_export=jointure_totale.copy()\n",
    "Fichier_export['Type de route']=Fichier_export.apply(lambda x : x['Routes'].split(' ')[0], axis=1)\n",
    "Fichier_export['NjRoute']=Fichier_export.apply(lambda x : x['Routes'].split(' ')[1], axis=1)\n",
    "Fichier_export.drop(['Id', 'Id_reel','Routes','Pr_origine_section', 'Lat_origine_section', 'Long_origine_section', 'Pr_fin_section',\n",
    "       'Lat_fin_section', 'Long_fin_section', 'Date_prise_effet', 'long','Lat', 'pr', 'abs', 'dep', 'route', 'nom_station_bdd', 'type_poste',\n",
    "       'dep_bdd', 'reseau', 'route_bdd', 'pr_bdd', 'abs_bdd', 'y_wgs', 'x_wgs', 'sens1', 'sens2', 'Id_reel_bdd'],axis=1, inplace=True)\n",
    "Fichier_export.rename(columns={'Nom':'Nom Station','Type':'Type Station', 'Dep':'Département', 'Pr_station':'PR Station'}, inplace=True)\n",
    "Fichier_export=Fichier_export[['Nom Station','Type Station','Département','Type de route','NjRoute','PR Station','Latitude','Longitude','Sens_1','Sens_2',\n",
    "               'annee', 'vl_sens_1', 'pl_sens_1', 'vl_sens_2','pl_sens_2']]\n",
    "Fichier_export=Fichier_export.reset_index().reset_index().drop('index',axis=1)\n",
    "Fichier_export['index_f']=Fichier_export.apply(lambda x : x['level_0']+2,axis=1)\n",
    "Fichier_export.drop('level_0',axis=1, inplace = True)\n",
    "Fichier_export.set_index('index_f', inplace=True)\n",
    "Fichier_export.loc[0]=np.NaN\n",
    "Fichier_export.loc[1]=np.NaN\n",
    "Fichier_export.sort_index(inplace=True)\n",
    "Fichier_export.to_csv(r'Q:\\DAIT\\TI\\DREAL33\\2019\\C19SA0035_OTR-NA\\GT_donnees_mobilite_collectivites\\insertion_donnees_iris\\exemple_fichier_tmja.csv',sep=';',\n",
    "                     index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
