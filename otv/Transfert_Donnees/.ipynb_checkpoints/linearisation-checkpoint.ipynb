{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEARISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "import agrege_troncon as at\n",
    "import Connexion_Transfert as ct\n",
    "import matplotlib, os, fiona\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sqlalchemy import Table, Column, Integer, String, Float,MetaData\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely.wkt import dumps\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD17\n",
    "Réutiliser les comptages importés grace au Notebooj Import_trafics (chap cd17) et les données de troncons elemnetaires issues du notebook agrege_troncon pour déterminer les nouveaux id_comptag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des données de troncon elem (à modifier par imporft depuis Bdd quand le transfert vers Bdd en sortie d'agreg_troncon sera calé)\n",
    "gdf_traf=gp.read_file(r'Q:\\DAIT\\TI\\DREAL33\\2018\\C18SA00003_OTR-linearisation_NA_(JM)\\Production\\Travail\\Donnees_produites\\fichiers_temp_travail\\troncelem2016_Bdt17_ed16_l.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des points de comptages depuis la Bdd\n",
    "with ct.ConnexionBdd('gti_otv') as c :\n",
    "    rqt=\"select * from comptage.na_2010_2017_p where dep='17' and geom is not null\"\n",
    "    gdf_pt_cpt = gp.read_postgis(rqt, c.connexionPsy)\n",
    "#mise en forme attributs\n",
    "def convert_tmja(df) : \n",
    "    for annee in [a for a in range(2017, 2009,-1)]+['autre'] :\n",
    "        attr='tmja_'+str(annee)\n",
    "        if ~np.isnan(df[attr]) : \n",
    "            return (df[attr], annee)\n",
    "df_adj=gdf_pt_cpt.apply(lambda x : convert_tmja(x), axis=1,result_type='expand')\n",
    "df_adj.columns=['tmja_recent','annee_tmja_recent']\n",
    "gdf_pt_cpt=gdf_pt_cpt.merge(df_adj, left_index=True, right_index=True)#[['id_comptag','geom','type_poste','tmja_recent','annee_tmja_recent' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtre des points ponctuels en été\n",
    "pt_pctuels_ete=gdf_pt_cpt.loc[gdf_pt_cpt['type_poste']=='ponctuel'].copy()\n",
    "def verif_ete(tmja_2015, obs_2015, tmja_2016, obs_2016) :\n",
    "    if tmja_2016 > 0 :\n",
    "        return any([a in ['July','August']  for a in [pd.to_datetime(obs_2016.split(',')[1].split('-')[i], format='%d/%m/%Y').month_name()for i in [0,1]]])\n",
    "    elif tmja_2015 > 0 :\n",
    "        return any([a in ['July','August']  for a in [pd.to_datetime(obs_2015.split(',')[1].split('-')[i], format='%d/%m/%Y').month_name()for i in [0,1]]])\n",
    "    else : return False\n",
    "pt_pctuels_ete['ete']=pt_pctuels_ete.apply(lambda x : verif_ete(x['tmja_2015'],x['obs_2015'], x['tmja_2016'], x['obs_2016']), axis=1)\n",
    "pt_pctuels_ete=pt_pctuels_ete.loc[~pt_pctuels_ete['ete']].copy()\n",
    "gdf_pt_cpt=gdf_pt_cpt.loc[gdf_pt_cpt.index.isin(pt_pctuels_ete.index.tolist()+gdf_pt_cpt.loc[gdf_pt_cpt['type_poste']!='ponctuel'].index.tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#croisement point et lignes (sur labase d'un buffer de 0.5m)\n",
    "gdf_pt_cpt.geometry=gdf_pt_cpt.buffer(0.5)\n",
    "gdf_traf_pt=gp.sjoin(gdf_traf,gdf_pt_cpt,how=\"left\", op='intersects' )[['id','id_ign','tmja', 'importance','numero','codevoie_d','sens','long_km','nature',\n",
    "            'id_comptag_left', 'id_tronc_e', 'id_comptag_right','type_poste','tmja_recent','annee_tmja_recent','source','target', 'geometry']].rename(\n",
    "columns={'id_comptag_left':'id_cpt_lin','id_comptag_right': 'id_cpt_pt','tmja':'tmja_lin'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recherch des tronc_elem avec plusieurs points\n",
    "gdf_traf_pt_notna=gdf_traf_pt.loc[~gdf_traf_pt['id_cpt_pt'].isna()].copy()\n",
    "tronc_elem_pt_cpt=gdf_traf_pt_notna.sort_index().groupby('id_tronc_e').agg({'id_cpt_pt' : lambda x : tuple(x),\n",
    "                                       'type_poste' : lambda x : tuple(x),\n",
    "                                        'tmja_recent' : lambda x : tuple(x),\n",
    "                                        'annee_tmja_recent' : lambda x : tuple(x)})\n",
    "trc_elem_multipt=tronc_elem_pt_cpt.loc[tronc_elem_pt_cpt.apply(lambda x : len(x['id_cpt_pt'])>1,axis=1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mise en forme des trocnon supportant plusieurs point : \n",
    "    #gestion des trocnon avec plusieurs points differntes (on garde le ponctuele de trafic max de l'annee la plus recente si que des ponctuels, sinon le tournant ou permanent)\n",
    "    #verif de la validite des points en comprant les valeurs de trafic entre elle: si un des trafics est sup 1000 et que la variation est sup 30 %, on n'affecte pas\n",
    "def cpt_final (type_poste,id_comptag_right,tmja_recent,annee_tmja_recent ): \n",
    "    tmja_recent=np.array(tmja_recent) \n",
    "    id_comptag_right=np.array(id_comptag_right)\n",
    "    mask_tmja = (tmja_recent==max(tmja_recent))\n",
    "    if all(np.unique(type_poste)=='ponctuel') :\n",
    "        mask_annee=np.array(annee_tmja_recent)==max(annee_tmja_recent)\n",
    "        if all(mask_annee):#il n'y a qu'une seule annee\n",
    "            if all(mask_tmja) : \n",
    "                return id_comptag_right[0]\n",
    "            else : \n",
    "                return id_comptag_right[mask_tmja][0]\n",
    "        else : #il y a pluseurs annee dc on ne garde que le max de la bonne annee\n",
    "            return id_comptag_right[mask_tmja]\n",
    "    else :\n",
    "        mask_type_poste=np.isin(np.array(type_poste),['permanent', 'tournant'])#on trouve le(s) poste(s) non ponctuel\n",
    "        return id_comptag_right[mask_type_poste][0]\n",
    "#verif de la validite des points en comprant les valeurs de trafic entre elle: si un des trafics est sup 1000 et que la variation est sup 30 %, on n'affecte pas\n",
    "trc_elem_multipt['id_cpt_final']=trc_elem_multipt.apply(lambda x: cpt_final(\n",
    "    x['type_poste'], x['id_cpt_pt'], x['tmja_recent'], x['annee_tmja_recent']), axis=1)\n",
    "trc_elem_multipt['valide']=trc_elem_multipt.apply(lambda x : (False if abs(100-(min(x['tmja_recent'])/max(x['tmja_recent'])*100))>30 and\n",
    "                                                                    max(x['tmja_recent'])>1000 and all(np.unique(x['type_poste'])=='ponctuel') else True), axis=1)\n",
    "trc_elem_multipt=trc_elem_multipt.loc[trc_elem_multipt['valide']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtention de la df des id_comptage_pt par tronc_elem\n",
    "tronc_elem_pt_cpt.update(trc_elem_multipt.drop('id_cpt_pt',axis=1).rename(columns={'id_cpt_final':'id_cpt_pt'})[['id_cpt_pt']])\n",
    "tronc_elem_pt_cpt['id_cpt_pt']=tronc_elem_pt_cpt.apply(lambda x : x['id_cpt_pt'][0] if (isinstance(x['id_cpt_pt'], tuple) or \n",
    "                                                       isinstance(x['id_cpt_pt'], np.ndarray)) else x['id_cpt_pt'],axis=1) #uniformisation du type de données en str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfert de l'id_cpt_pt final dans la df source\n",
    "gdf_traf_pt_final=gdf_traf_pt[['id_ign','importance','numero', 'id_cpt_lin','tmja_lin', 'id_tronc_e','geometry','source','target']].merge(tronc_elem_pt_cpt[['id_cpt_pt']], how='left',left_on='id_tronc_e', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour info, ligne ign non linéarisée qui le deviennent\n",
    "gdf_creation_idcpt=gdf_traf_pt_final.loc[(gdf_traf_pt_final['id_cpt_lin'].isna()) & (~gdf_traf_pt_final['id_cpt_pt'].isna())].copy()\n",
    "#pour info, ligne ign linéarisée qui le sont toujours, mais peut avec un nouvel id_ign\n",
    "gdf_variation_idcpt=gdf_traf_pt_final.loc[(~gdf_traf_pt_final['id_cpt_lin'].isna()) & (~gdf_traf_pt_final['id_cpt_pt'].isna()) & (gdf_traf_pt_final['id_cpt_pt']!=gdf_traf_pt_final['id_cpt_lin'])].sort_values('id_cpt_lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mettre à jour les lignes des troncons qui appartiennent à des id_comptag qui supporte un nouvel id_cpt\n",
    "**IL FAUT METTRE EN FORME LES 2 BLOCS CI-DESSSOUS EN FONCTION DANS LES MODULES QUI VONT BIEN <br> <br> ATTENTION DES ERREURS DE LINEARISATION EN 2016 :** <br>\n",
    "- propagation non arreté à la 1èer intersection mais à la deuxième (à l'Est de l'id_comptag) : TRONROUT0000000032986160 linearisee avec 17-D129-57+510\n",
    "- rd point linéarisé mais aucun autre troncon avec lui : 17-D202-9+950 tout seul sur le rd point entouré par du 17-D202-10+400 TRONROUT0000000032864482\n",
    "- affectation des points aux lignes : vérifier qu'au moins 1 lignes dans la distance eéquivalente au buffer présete bien le même numero de voie que le compteur\n",
    "TRONROUT0000000032877685 avec 17-D107-62+282 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f df_linearisee_fin_glob\n",
    "#on ne prend que les points de comptages présentant le mm nom de voie\n",
    "gdf_variation_idcpt_ok=gdf_variation_idcpt.loc[gdf_variation_idcpt.apply(lambda x : \n",
    "                        x['id_cpt_lin'].split('-')[1] == x['id_cpt_pt'].split('-')[1],axis=1)].copy()\n",
    "\n",
    "#isoler les lignes qui sont relaticves à l'id_cpt_lin et trouver tout les points concerné. si que 1 point de type permanent on passe\n",
    "for id_cpt_a_test in gdf_variation_idcpt_ok.id_cpt_lin.unique() : \n",
    "    print(id_cpt_a_test)\n",
    "#id_cpt_a_test='17-D939-88+0'\n",
    "    lgn_idcpt_lin=gdf_traf.loc[gdf_traf['id_comptag']==id_cpt_a_test].copy()\n",
    "    liste_new_pt=gdf_variation_idcpt_ok.loc[gdf_variation_idcpt_ok['id_cpt_lin']==id_cpt_a_test].id_cpt_pt.unique()\n",
    "    if (gdf_pt_cpt.loc[gdf_pt_cpt.id_comptag.isin(liste_new_pt)].type_poste=='permanent').all() :\n",
    "        print('que permanent')\n",
    "        continue\n",
    "\n",
    "    #trouver la ligne la plus proche relative à chaque nouveau point\n",
    "    gdf_variation_idcpt_ok_geom=gdf_variation_idcpt_ok.merge(gdf_pt_cpt[['id_comptag', 'geom']], left_on='id_cpt_pt', right_on='id_comptag').rename(columns={\n",
    "        'geom' : 'geom_id_cpt_pt'}) # pour trouver la ligne qui suppotrt ele pt j'ai besoin da le distance au pt de comptage\n",
    "    gdf_variation_idcpt_ok_geom['dist_id_cpt']=gdf_variation_idcpt_ok_geom.apply(lambda x : x['geometry'].distance(x['geom_id_cpt_pt']), axis=1)\n",
    "\n",
    "    t1=gdf_variation_idcpt_ok_geom.loc[gdf_variation_idcpt_ok_geom['id_cpt_pt'].isin(liste_new_pt)]\n",
    "    df_lgn_proche_cpt=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']]\n",
    "    #si la distance minimale entre le nouveau point et le trocnon qui le supporte est sup à 0.5 (paramètre du buffer au dessus) ça veut dire que pb, dc on passe\n",
    "    if (df_lgn_proche_cpt.dist_id_cpt>0.5).all() : \n",
    "        print('pb distance')\n",
    "        continue\n",
    "\n",
    "    #trouver les lignes qui intersectent celles de l'idcompatg etudie dont l'importance est = ou sup\n",
    "        #list des source / target de l'idcomptage\n",
    "    list_src_tgt=lgn_idcpt_lin.source.tolist()+lgn_idcpt_lin.target.tolist()\n",
    "    df_lgn_interscts=gdf_traf.loc[((gdf_traf['source'].isin(list_src_tgt)) | (gdf_traf['target'].isin(list_src_tgt))) &\n",
    "                                    (~gdf_traf.index.isin(lgn_idcpt_lin.index.tolist()))].copy()\n",
    "    df_lgn_interscts_merge=gp.sjoin(df_lgn_interscts,lgn_idcpt_lin, op='intersects')\n",
    "    df_lgn_interscts_filtre=df_lgn_interscts_merge.loc[df_lgn_interscts_merge.apply(lambda x : int(x['importance_left']) <= int(x['importance_right']), axis=1)].copy()\n",
    "    nom_attribut=[a for a  in df_lgn_interscts_filtre.columns if a[-4:]=='left']\n",
    "    df_lgn_interscts_fin=df_lgn_interscts_filtre.rename(columns={n : n[:-5] for n in nom_attribut})[[c for c in df_lgn_interscts.columns]]\n",
    "\n",
    "    #faire un graph avec les lignes de l'id_cpt_lin et celles qui interectent avec la bonne importance\n",
    "        #regrouper l'ensemble des lignes dans une df \n",
    "    ligne_pr_graph=gdf_traf.loc[gdf_traf['id_ign'].isin(df_lgn_interscts.id_ign.tolist()+lgn_idcpt_lin.id_ign.tolist())]\n",
    "    ligne_pr_graph_transfert=ligne_pr_graph.copy()\n",
    "    ligne_pr_graph_transfert['geom']=ligne_pr_graph_transfert.apply(lambda x : dumps(x.geometry), axis=1)\n",
    "\n",
    "    #preparer le lineaire pour graph\n",
    "    ligne_pr_graph=pd.concat([df_lgn_interscts_fin, lgn_idcpt_lin], sort=False, axis=0)\n",
    "        #gestion des doucblons id_ign\n",
    "    ligne_pr_graph.drop_duplicates('id_ign', inplace=True)\n",
    "        #si doublons id restant on remplace l'id\n",
    "    if ~ligne_pr_graph.loc[ligne_pr_graph.duplicated('id', keep=False)].empty : \n",
    "        ligne_pr_graph=ligne_pr_graph.reset_index().drop('index', axis=1).copy()\n",
    "        ligne_pr_graph.id=ligne_pr_graph.index\n",
    "    ligne_pr_graph_transfert=ligne_pr_graph.copy()\n",
    "    ligne_pr_graph_transfert['geom']=ligne_pr_graph_transfert.apply(lambda x : dumps(x.geometry), axis=1)\n",
    "\n",
    "    #faire un graph à partir de ces lignes\n",
    "    bdd='gti_otv'\n",
    "    schema='public'\n",
    "    table='graph_temp'\n",
    "    table_vertex='graph_temp_vertices_pgr'\n",
    "    with ct.ConnexionBdd(bdd) as c:\n",
    "        metadata = MetaData(schema=schema)\n",
    "        #supprimer table si elle existe\n",
    "        rqt=f\"drop table if exists {schema}.{table} ; drop table if exists {schema}.{table_vertex} \"\n",
    "        c.sqlAlchemyConn.execute(rqt)\n",
    "\n",
    "        #creer nouvelle table\n",
    "        graph = Table('graph_temp', metadata,\n",
    "            Column('id', Integer, primary_key=True),\n",
    "            Column('id_ign', String),\n",
    "            Column('importance', String),\n",
    "            Column('numero', String),\n",
    "            Column('nature', String),\n",
    "            Column('codevoie_d', String),\n",
    "            Column('sens', String),\n",
    "            Column('source', Integer),\n",
    "            Column('target', Integer),\n",
    "            Column('long_km', Float),\n",
    "            Column('geom',Geometry()))\n",
    "        metadata.create_all(c.engine)\n",
    "        ligne_pr_graph_transfert[['id','id_ign','importance','numero','nature','codevoie_d','source','sens','target','long_km','geom']].to_sql(\n",
    "            table,c.sqlAlchemyConn,schema=schema,if_exists='append', index=False )\n",
    "        #creer le graph\n",
    "        rqt_creation_graph=f\"\"\"update {schema}.{table} set geom=st_Multi(st_setsrid(geom,2154)), source=null, target=null ; \n",
    "                             select pgr_createTopology('{schema}.{table}', 0.001,'geom')\"\"\"\n",
    "        c.sqlAlchemyConn.execute(rqt_creation_graph)\n",
    "        rqt_anlyse_graph=f\"SELECT pgr_analyzeGraph('{schema}.{table}', 0.001,\\'geom\\')\"\n",
    "        c.curs.execute(rqt_anlyse_graph)#je le fait avec psycopg2 car avec sql acchemy ça ne passe pas\n",
    "        c.connexionPsy.commit()\n",
    "\n",
    "    #appel des fonction de carac des lignes \n",
    "    df=at.import_donnes_base('gti_otv','public', 'graph_temp','graph_temp_vertices_pgr')\n",
    "    df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "    df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=at.identifier_rd_pt(df)\n",
    "    df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "\n",
    "    #ensuite l'idée c'est de créer un dico avec pr chauqe point de comptage que l'on souhaite lineariser le recenesmeent de  : \n",
    "    #la ligne qui supporte le pt, le numero du tronc elem, les lignes qui compose le tronc elem\n",
    "    #on faitd'abords les points ponctuels, puis le permanent pour pouvoir les ordonner avec un drapeau\n",
    "    dico_ligne_pt_base=df_lgn_proche_cpt.drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                            'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "    dico_ligne_pt={k[0] : {'ligne_base': k[1],'trc_elem':k[2],'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==k[2]) & \n",
    "                                                                            (gdf_traf_pt.id_ign.isin(lgn_idcpt_lin.id_ign.tolist()))].id_ign.tolist(), 'priorite':2} \n",
    "                   for k in dico_ligne_pt_base}\n",
    "    dico_ligne_pt[id_cpt_a_test]={'ligne_base' : gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                                (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_ign.values[0],\n",
    "                                 'trc_elem':gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(['permanent','tournant'])) & \n",
    "                                                                (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0], \n",
    "                                 'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==gdf_traf_pt.loc[(gdf_traf_pt['type_poste'].isin(\n",
    "                                     ['permanent','tournant'])) & (gdf_traf_pt['id_cpt_lin']==id_cpt_a_test)].id_tronc_e.values[0]) & \n",
    "                   (gdf_traf_pt['id_ign'].isin(df_lignes.index.to_list()))].id_ign.tolist(),\n",
    "                                 'priorite':1}\n",
    "        #dico des lignes relatives a un autre comptage\n",
    "    dico_lign_cpt_pct={k:[a for kd in dico_ligne_pt.keys() if kd!=k for a in dico_ligne_pt[kd]['lgn_trc_elem']] for k in dico_ligne_pt.keys()}\n",
    "\n",
    "    #attention, la fonction at.regrouper_troncon ne marche pas si la ligne de base est un rd point.\n",
    "    #il faut dc vérifier ce point et dans le cas où ça arrive prendre un ligne des lgn_trc_elem qui ne soit pas sur un rdpoint\n",
    "    if not carac_rd_pt.empty : \n",
    "        list_id_ign_rd_pt=[a for l_rdpt in carac_rd_pt.id_ign_rdpt.tolist() for a in l_rdpt]\n",
    "        for k in dico_ligne_pt.keys() : \n",
    "            if dico_ligne_pt[k]['ligne_base'] in list_id_ign_rd_pt : \n",
    "                dico_ligne_pt[k]['ligne_base']=[e for e in dico_ligne_pt[k]['lgn_trc_elem'] if e not in list_id_ign_rd_pt][0]\n",
    "\n",
    "    #pour chaque ligne : on fait tourner la fonction de tronc elem à partir du graph avec les importance sup ou égale, \n",
    "    #d'abord pour le cpt permanent, ensuite pour les autres, avec prise necompte des lignes des tronc elem des autres compteurs. \n",
    "        #list des id_ign des tronc_elem\n",
    "    liste_id_ign_te=[a  for k, v in dico_ligne_pt.items() for a in v['lgn_trc_elem']]\n",
    "    lignes_traitees=[]\n",
    "        #utilisret les fnctions de carac\n",
    "    try : \n",
    "        for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])): \n",
    "            ligne_comp=[x for x in at.regrouper_troncon([dico_ligne_pt[k]['ligne_base']],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "                                                       dico_lign_cpt_pct[k]+lignes_traitees)[0].id.tolist() \n",
    "                        if x in df_lignes.index.tolist()]\n",
    "            #print(f\"id_cpt : {k},'\\n',lignes_traitees : {lignes_traitees},'\\n', lignes de base : {ligne_comp}\")\n",
    "            lignes_traitees+=ligne_comp\n",
    "            dico_ligne_pt[k]['lignes_comp_base']=[a for a in ligne_comp if a in lgn_idcpt_lin.id_ign.to_list()]\n",
    "    except KeyError : \n",
    "        print(f'pb sur id_comptag : {id_cpt_a_test}')\n",
    "\n",
    "    #a ce stade, si le cpt perm est séparé par une voie d'importance sup ou =, il y a discontinuité, dc il faut \n",
    "        #-relevé les discontinuite\n",
    "    ids_affectes=[a for k in dico_ligne_pt.keys() for a in dico_ligne_pt[k]['lignes_comp_base']+dico_ligne_pt[k]['lgn_trc_elem']]\n",
    "    df_non_affecte=lgn_idcpt_lin.loc[~lgn_idcpt_lin.id_ign.isin(ids_affectes)]\n",
    "\n",
    "    #tant que des discont existent\n",
    "    while not df_non_affecte.empty : \n",
    "        for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])):\n",
    "            #discontinuité qui touche le cpt : \n",
    "            list_ids_cpt=list(set(dico_ligne_pt[k]['lignes_comp_base']+dico_ligne_pt[k]['lgn_trc_elem']))\n",
    "            src_tgt_affecte=(lgn_idcpt_lin.loc[lgn_idcpt_lin.id_ign.isin(list_ids_cpt)].source.tolist() + \n",
    "                    lgn_idcpt_lin.loc[lgn_idcpt_lin.id_ign.isin(list_ids_cpt)].target.tolist() )\n",
    "            ids_tch_cpt=df_non_affecte.loc[(df_non_affecte.source.isin(src_tgt_affecte)) | (df_non_affecte.target.isin(src_tgt_affecte))].id_ign.tolist()\n",
    "            if not ids_tch_cpt :\n",
    "                continue\n",
    "            print (k, ids_tch_cpt)\n",
    "            #pour chaque ligne on récupère tout les troncons elem et on ajoute au dico\n",
    "            try : \n",
    "                ids_a_ajouter=[a for b in [at.regrouper_troncon([e],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "                 ids_affectes)[0].id.tolist() for e in ids_tch_cpt] for a in b]\n",
    "            except at.PasAffectationError :#dans le cas par exemple où tout les lignes qui restent sont des rd ppoint\n",
    "                if (~df_non_affecte.id_rdpt.isna()).all() and len(df_non_affecte.id_rdpt.unique())==1 : \n",
    "                    ids_a_ajouter=df_non_affecte.id_ign.tolist()\n",
    "            dico_ligne_pt[k]['lignes_comp_base']+=ids_a_ajouter\n",
    "            ids_affectes=[a for k in dico_ligne_pt.keys() for a in dico_ligne_pt[k]['lignes_comp_base']+dico_ligne_pt[k]['lgn_trc_elem']]\n",
    "            df_non_affecte=lgn_idcpt_lin.loc[~lgn_idcpt_lin.id_ign.isin(ids_affectes)]\n",
    "\n",
    "    # mettre en forme et creation df de correspondance finale : \n",
    "    df_linearisee=pd.DataFrame(index=dico_ligne_pt.keys(), data=dico_ligne_pt.values())\n",
    "    df_linearisee['lignes_comp_fin']=df_linearisee.apply(lambda x : x['lignes_comp_base'] + x['lgn_trc_elem'], axis=1 )\n",
    "    dico_inverse=df_linearisee[['lignes_comp_fin']].to_dict()\n",
    "    dico_inverse={a:k   for k, val  in dico_inverse['lignes_comp_fin'].items() for a in val}\n",
    "    df_linearisee_fin=pd.DataFrame(index=dico_inverse.keys(), data=dico_inverse.values(), columns=['id_cpt_fin'])\n",
    "\n",
    "    try : \n",
    "        df_linearisee_fin_glob=pd.concat([df_linearisee_fin_glob,df_linearisee_fin], axis=0, sort=False)\n",
    "    except NameError : \n",
    "        df_linearisee_fin_glob=df_linearisee_fin.copy()\n",
    "df_linearisee_fin_glob.to_csv(r'Q:\\DAIT\\TI\\DREAL33\\2018\\C18SA00003_OTR-linearisation_NA_(JM)\\Production\\Travail\\Donnees_produites\\fichiers_temp_travail\\test_lin_pt_existant.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pour les nouveaux points à linéariser \n",
    "**L'idée c'est d'isoler la voie concernée, en ne conservant que le troncons qui n'ont pas d'id_comptage. ensuite on fait le tour de tout les pt dispo, on crée un graph sur la base du numero de voie et des troncon qui intersectent et qui soit ont une importance sup ou =, soit sont compté. Puis on propage sur ce graph** <br> <br>\n",
    "**ATTENTION DES ERREURS DE LINEARISATION EN 2016 :** <br>\n",
    "- propagation non arreté à la 1èer intersection mais à la deuxième (à l'Est de l'id_comptag) : TRONROUT0000000032986160 linearisee avec 17-D129-57+510\n",
    "- rd point linéarisé mais aucun autre troncon avec lui : 17-D202-9+950 tout seul sur le rd point entouré par du 17-D202-10+400 TRONROUT0000000032864482\n",
    "- affectation des points aux lignes : vérifier qu'au moins 1 lignes dans la distance eéquivalente au buffer présete bien le même numero de voie que le compteur\n",
    "TRONROUT0000000032877685 avec 17-D107-62+282 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f df_linearisee_fin_glob\n",
    "\n",
    "#on ne garde que les points relatif à leur voie\n",
    "gdf_creation_idcpt_ok=gdf_creation_idcpt.loc[gdf_creation_idcpt.apply(lambda x : x['numero']==x['id_cpt_pt'].split('-')[1],axis=1)].copy()\n",
    "\n",
    "#on isole un point de comptage et on trouve tout les points relatifs à cette voie : \n",
    "pt_traites=[]\n",
    "for id_cpt_a_test in gdf_creation_idcpt_ok.id_cpt_pt.unique() :\n",
    "    #drapeau si point deja traite dans le cadre d'un autre point sur la mm voie\n",
    "    print(id_cpt_a_test)\n",
    "    if id_cpt_a_test in pt_traites : \n",
    "        continue\n",
    "    numero=id_cpt_a_test.split('-')[1]\n",
    "    df_tt_cpt_route=gdf_creation_idcpt_ok.loc[gdf_creation_idcpt_ok['numero']==numero]\n",
    "    liste_new_pt=df_tt_cpt_route.id_cpt_pt.unique()\n",
    "\n",
    "    #on recherche la ligne la plus proche pour cahque pt de comptage\n",
    "    gdf_creation_idcpt_ok_geom=df_tt_cpt_route.merge(gdf_pt_cpt[['id_comptag', 'geom']], \n",
    "                left_on='id_cpt_pt', right_on='id_comptag').rename(columns={'geom' : 'geom_id_cpt_pt'})\n",
    "    gdf_creation_idcpt_ok_geom['dist_id_cpt']=gdf_creation_idcpt_ok_geom.apply(lambda x : x['geometry'].distance(x['geom_id_cpt_pt']), axis=1)\n",
    "\n",
    "    t1=gdf_creation_idcpt_ok_geom.loc[gdf_creation_idcpt_ok_geom['id_cpt_pt'].isin(liste_new_pt)]\n",
    "    df_lgn_proche_cpt=t1.loc[t1.groupby(['id_cpt_pt'])['dist_id_cpt'].transform(min)==t1['dist_id_cpt']]\n",
    "    \n",
    "    #toute les lignes du numero de voie\n",
    "    df_troncon_numero=gdf_traf.loc[gdf_traf['numero']==numero]\n",
    "    #sans les lignes qui ont déjà un id_comptag\n",
    "    df_troncon_numero=df_troncon_numero.loc[df_troncon_numero['id_comptag'].isna()].copy()\n",
    "    list_src_tgt_tronc_num=df_troncon_numero.source.tolist() + df_troncon_numero.target.tolist()\n",
    "    \n",
    "    #trouver les lignes d'importance sup ou = qui intersectent\n",
    "    df_lgn_interscts=gdf_traf.loc[((gdf_traf['source'].isin(list_src_tgt_tronc_num)) | (gdf_traf['target'].isin(list_src_tgt_tronc_num))) &\n",
    "                                    (~gdf_traf.index.isin(df_troncon_numero.index.tolist())) & (gdf_traf['importance']!='NC')].copy()\n",
    "    df_lgn_interscts_merge=gp.sjoin(df_lgn_interscts,df_troncon_numero, op='intersects')\n",
    "    df_lgn_interscts_filtre=df_lgn_interscts_merge.loc[df_lgn_interscts_merge.apply(lambda x : int(x['importance_left']) <= int(x['importance_right']), axis=1)].copy()\n",
    "    nom_attribut=[a for a  in df_lgn_interscts_filtre.columns if a[-4:]=='left']\n",
    "    df_lgn_interscts_fin=df_lgn_interscts_filtre.rename(columns={n : n[:-5] for n in nom_attribut})[[c for c in df_lgn_interscts.columns]]\n",
    "    #trouver les lignes avec un id_comptag qu iintersectent\n",
    "    df_lgn_id_cpt_lin=gdf_traf.loc[~gdf_traf.id_comptag.isna()].copy()\n",
    "    df_lgn_id_cpt_lin=df_lgn_id_cpt_lin.loc[(df_lgn_id_cpt_lin['source'].isin(list_src_tgt_tronc_num)) | \n",
    "                                            (df_lgn_id_cpt_lin['target'].isin(list_src_tgt_tronc_num))].copy()\n",
    "    \n",
    "    #preparer le lineaire pour graph\n",
    "    ligne_pr_graph=pd.concat([df_lgn_interscts_fin, df_lgn_id_cpt_lin,df_troncon_numero], sort=False, axis=0)\n",
    "    ligne_pr_graph.drop_duplicates('id_ign', inplace=True)\n",
    "        #si doublons id restant on remplace l'id\n",
    "    if ~ligne_pr_graph.loc[ligne_pr_graph.duplicated('id', keep=False)].empty : \n",
    "        ligne_pr_graph=ligne_pr_graph.reset_index().drop('index', axis=1).copy()\n",
    "        ligne_pr_graph.id=ligne_pr_graph.index\n",
    "    ligne_pr_graph_transfert=ligne_pr_graph.copy()\n",
    "    ligne_pr_graph_transfert['geom']=ligne_pr_graph_transfert.apply(lambda x : dumps(x.geometry), axis=1)\n",
    "    \n",
    "    bdd='gti_otv'\n",
    "    schema='public'\n",
    "    table='graph_temp'\n",
    "    table_vertex='graph_temp_vertices_pgr'\n",
    "    with ct.ConnexionBdd(bdd) as c:\n",
    "        metadata = MetaData(schema=schema)\n",
    "        #supprimer table si elle existe\n",
    "        rqt=f\"drop table if exists {schema}.{table} ; drop table if exists {schema}.{table_vertex} \"\n",
    "        c.sqlAlchemyConn.execute(rqt)\n",
    "\n",
    "        #creer nouvelle table\n",
    "        graph = Table('graph_temp', metadata,\n",
    "            Column('id', Integer, primary_key=True),\n",
    "            Column('id_ign', String),\n",
    "            Column('importance', String),\n",
    "            Column('numero', String),\n",
    "            Column('nature', String),\n",
    "            Column('codevoie_d', String),\n",
    "            Column('sens', String),\n",
    "            Column('source', Integer),\n",
    "            Column('target', Integer),\n",
    "            Column('long_km', Float),\n",
    "            Column('geom',Geometry()))\n",
    "        metadata.create_all(c.engine)\n",
    "        ligne_pr_graph_transfert[['id','id_ign','importance','numero','nature','codevoie_d','source','sens','target','long_km','geom']].to_sql(\n",
    "            table,c.sqlAlchemyConn,schema=schema,if_exists='append', index=False )\n",
    "        #creer le graph\n",
    "        rqt_creation_graph=f\"\"\"update {schema}.{table} set geom=st_Multi(st_setsrid(geom,2154)), source=null, target=null ; \n",
    "                             select pgr_createTopology('{schema}.{table}', 0.001,'geom')\"\"\"\n",
    "        c.sqlAlchemyConn.execute(rqt_creation_graph)\n",
    "        rqt_anlyse_graph=f\"SELECT pgr_analyzeGraph('{schema}.{table}', 0.001,\\'geom\\')\"\n",
    "        c.curs.execute(rqt_anlyse_graph)#je le fait avec psycopg2 car avec sql acchemy ça ne passe pas\n",
    "        c.connexionPsy.commit()\n",
    "        \n",
    "        #appel des fonction de carac des lignes \n",
    "    df=at.import_donnes_base('gti_otv','public', 'graph_temp','graph_temp_vertices_pgr')\n",
    "    df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "    df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=at.identifier_rd_pt(df)\n",
    "    df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "    \n",
    "    #creation d'un dico des ptde base\n",
    "    dico_ligne_pt_base=df_lgn_proche_cpt.drop_duplicates(['id_cpt_lin','id_cpt_pt' ])[['id_cpt_pt',\n",
    "                            'id_ign','id_tronc_e']].set_index('id_cpt_pt').to_records()\n",
    "    dico_ligne_pt={k[0] : {'ligne_base': k[1],'trc_elem':k[2],'lgn_trc_elem' : gdf_traf_pt.loc[(gdf_traf_pt['id_tronc_e']==k[2]) & \n",
    "                                                                                (gdf_traf_pt.id_ign.isin(df_troncon_numero.id_ign.tolist()))].id_ign.tolist()\n",
    "                           , 'priorite':2} \n",
    "                       for k in dico_ligne_pt_base}\n",
    "    \n",
    "    #dico des lignes relatives a un autre comptage\n",
    "    dico_lign_cpt_pct={k:[a for kd in dico_ligne_pt.keys() if kd!=k for a in dico_ligne_pt[kd]['lgn_trc_elem']] for k in dico_ligne_pt.keys()}\n",
    "    \n",
    "    #attention, la fonction at.regrouper_troncon ne marche pas si la ligne de base est un rd point.\n",
    "    #il faut dc vérifier ce point et dans le cas où ça arrive prendre un ligne des lgn_trc_elem qui ne soit pas sur un rdpoint\n",
    "    if not carac_rd_pt.empty : \n",
    "        list_id_ign_rd_pt=[a for l_rdpt in carac_rd_pt.id_ign_rdpt.tolist() for a in l_rdpt]\n",
    "        for k in dico_ligne_pt.keys() : \n",
    "            if dico_ligne_pt[k]['ligne_base'] in list_id_ign_rd_pt : \n",
    "                dico_ligne_pt[k]['ligne_base']=[e for e in dico_ligne_pt[k]['lgn_trc_elem'] if e not in list_id_ign_rd_pt][0]\n",
    "                \n",
    "    #pour chaque ligne : on fait tourner la fonction de tronc elem à partir du graph avec les importance sup ou égale, \n",
    "    #d'abord pour le cpt permanent, ensuite pour les autres, avec prise necompte des lignes des tronc elem des autres compteurs. \n",
    "        #list des id_ign des tronc_elem\n",
    "    liste_id_ign_te=[a  for k, v in dico_ligne_pt.items() for a in v['lgn_trc_elem']]\n",
    "    lignes_traitees=[]\n",
    "        #utilisret les fnctions de carac\n",
    "    try : \n",
    "        for k in sorted(dico_ligne_pt.keys(), key=lambda x: (dico_ligne_pt[x]['priorite'])): \n",
    "            ligne_comp=[x for x in at.regrouper_troncon([dico_ligne_pt[k]['ligne_base']],df_avec_rd_pt,carac_rd_pt, df2_chaussees,\n",
    "                                                       dico_lign_cpt_pct[k]+lignes_traitees)[0].id.tolist() \n",
    "                        if x in df_lignes.index.tolist()]\n",
    "            #print(f\"id_cpt : {k},'\\n',lignes_traitees : {lignes_traitees},'\\n', lignes de base : {ligne_comp}\")\n",
    "            lignes_traitees+=ligne_comp\n",
    "            dico_ligne_pt[k]['lignes_comp_base']=[a for a in ligne_comp if a in df_troncon_numero.id_ign.to_list()]\n",
    "    except KeyError : \n",
    "        print(f'pb sur id_comptag : {id_cpt_a_test}')\n",
    "        \n",
    "    # mettre en forme et creation df de correspondance finale : \n",
    "    df_linearisee=pd.DataFrame(index=dico_ligne_pt.keys(), data=dico_ligne_pt.values())\n",
    "    df_linearisee['lignes_comp_fin']=df_linearisee.apply(lambda x : x['lignes_comp_base'] + x['lgn_trc_elem'], axis=1 )\n",
    "    dico_inverse=df_linearisee[['lignes_comp_fin']].to_dict()\n",
    "    dico_inverse={a:k   for k, val  in dico_inverse['lignes_comp_fin'].items() for a in val}\n",
    "    df_linearisee_fin=pd.DataFrame(index=dico_inverse.keys(), data=dico_inverse.values(), columns=['id_cpt_fin'])\n",
    "\n",
    "    try : \n",
    "        df_linearisee_fin_glob=pd.concat([df_linearisee_fin_glob,df_linearisee_fin], axis=0, sort=False)\n",
    "    except NameError : \n",
    "        df_linearisee_fin_glob=df_linearisee_fin.copy()\n",
    "        \n",
    "    pt_traites+=list(dico_ligne_pt.keys())\n",
    "df_linearisee_fin_glob.to_csv(r'Q:\\DAIT\\TI\\DREAL33\\2018\\C18SA00003_OTR-linearisation_NA_(JM)\\Production\\Travail\\Donnees_produites\\fichiers_temp_travail\\test_lin_noveau_pt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
